{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a79cf6c",
   "metadata": {},
   "source": [
    "# 🚀 Advanced RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "## 📘 개요\n",
    "**Advanced RAG**는 기본 RAG보다 한 단계 발전된 구조로,  \n",
    "단순히 “검색 → 문맥 삽입 → 답변 생성”의 흐름을 넘어  \n",
    "**정확도**, **효율성**, **확장성**을 개선하기 위한 기술들이 결합된 형태입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 기본 RAG 복습\n",
    "\n",
    "### 🔹 기본 구조\n",
    "1. **Query Embedding**: 사용자의 질문을 벡터로 변환  \n",
    "2. **Vector Search**: 벡터 DB에서 유사 문서 검색  \n",
    "3. **Context Injection**: 검색된 텍스트를 프롬프트에 추가  \n",
    "4. **LLM Generation**: LLM이 답변 생성\n",
    "\n",
    "### ⚠️ 한계점\n",
    "- 검색된 문서 중 진짜 관련 정보만 선별하기 어려움  \n",
    "- 검색 쿼리가 모호할 경우 적절한 문서가 검색되지 않음  \n",
    "- LLM의 답변이 실제 근거 문서와 다를 수 있음 (hallucination)  \n",
    "- 긴 문서나 다중 hop 질의에 대응하기 어려움\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Advanced RAG의 주요 구성요소\n",
    "\n",
    "### 1️⃣ Query Rewriting (질문 개선)\n",
    "- LLM이 사용자의 질문을 **검색 친화적으로 재작성**\n",
    "- 예시  \n",
    "  - 사용자: “그 회사 매출 어땠어?”  \n",
    "  - 변환: “2023년 삼성전자의 연간 매출액”\n",
    "- LangChain 예시: `MultiQueryRetriever`, `ContextualCompressionRetriever`\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Re-ranking (재정렬)\n",
    "- 벡터 검색 결과를 LLM 또는 Cross-Encoder로 재평가하여  \n",
    "  **가장 관련도가 높은 문서만 남김**\n",
    "- 예시:  \n",
    "  - 상위 20개 문서 → LLM 기반 재평가 → 상위 5개 사용\n",
    "- 도구 예시: `bge-reranker`, `Cohere Rerank API`, LangChain `ReRanker`\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Context Compression / Summarization\n",
    "- 너무 긴 문서를 그대로 LLM에 넣지 않고  \n",
    "  **요약(summarization)** 또는 **핵심 문장만 추출(extractive compression)**  \n",
    "- LangChain: `ContextualCompressionRetriever`, `LLMChainExtractor`\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Multi-hop Retrieval\n",
    "- 한 질문을 여러 하위 질문으로 나누어 **단계적 검색** 수행  \n",
    "- 예시:  \n",
    "  1. “AI 반도체 주요 기업은?”  \n",
    "  2. “NVIDIA의 매출 구조는?”\n",
    "- 대표 기법: **Self-Ask**, **ReAct**, **Chain-of-Thought RAG**\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Hybrid Retrieval (혼합 검색)\n",
    "- **벡터 검색 + 키워드 검색 + 메타데이터 필터링**을 결합  \n",
    "- 예시:  \n",
    "  `vector + keyword + date filter`\n",
    "- 장점: 의미적 유사성과 시기적 제약을 함께 반영  \n",
    "- Milvus: `hybrid_search()` 기능 활용 가능\n",
    "\n",
    "---\n",
    "\n",
    "### 6️⃣ Structured Retrieval / Graph RAG\n",
    "- 문서 간 관계를 **그래프(Neo4j, ArangoDB 등)** 로 저장  \n",
    "- LLM이 그래프 탐색을 통해 의미적 연결성을 찾아냄  \n",
    "- 예시:  \n",
    "  - 논문 인용 관계, FAQ 연결 구조 등  \n",
    "- 장점: 논리적 reasoning과 근거 추적에 강함\n",
    "\n",
    "---\n",
    "\n",
    "### 7️⃣ Response Verification & Grounding\n",
    "- LLM이 생성한 답변을 **다시 검증**하여 실제 근거 문서와 일치하는지 확인  \n",
    "- 방법  \n",
    "  - LLM이 스스로 “이 답변이 문서 내용과 일치합니까?” 재질문  \n",
    "  - 출처(Citation) 자동 추가  \n",
    "  - Self-RAG (Meta, 2024): 모델이 자체적으로 “retrieve → answer → verify → revise” 수행\n",
    "\n",
    "---\n",
    "\n",
    "### 8️⃣ Caching / Memory / Feedback Loop\n",
    "- 과거의 질의응답을 저장하여 **속도 향상 및 재사용**  \n",
    "- 오답 시 LLM이 피드백을 학습하도록 구성  \n",
    "- 예시: `LangChain Memory`, `VectorCache`, `User Feedback Loop`\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Advanced RAG 파이프라인 예시\n",
    "\n",
    "```text\n",
    "사용자 질문\n",
    "   ↓\n",
    "**질문 재작성 (Query Rewriter)** \n",
    "   ↓\n",
    "**Re-ranking (LLM 또는 Cross-Encoder)**\n",
    "   ↓\n",
    "Context Summarization (압축)\n",
    "   ↓\n",
    "LLM 답변 생성\n",
    "   ↓\n",
    "Answer Verification (Self-RAG)\n",
    "   ↓\n",
    "결과 + 출처 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9050bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus Collections: ['kmu_llm']\n",
      "Collection stats: {'row_count': 206}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pymilvus import MilvusClient\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ── 설정 ──\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY 환경 변수가 필요합니다.\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, openai_api_key=OPENAI_API_KEY)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Milvus 연결\n",
    "db_dir = '/Users/n-jison/Desktop/jaeig/kmu-agent-course/AI_Agent/retrieve'\n",
    "db_name = 'kmu.db'\n",
    "db_url = os.path.join(db_dir, db_name)\n",
    "client = MilvusClient(db_url)\n",
    "\n",
    "# 연결 확인\n",
    "collections = client.list_collections()\n",
    "print(\"Milvus Collections:\", collections)\n",
    "stats = client.get_collection_stats(collection_name='kmu_llm')\n",
    "print(\"Collection stats:\", stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc71b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advanced RAG 파이프라인 구현\n",
    "def rewrite_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Query Rewriter: 질문을 검색에 유리한 형태로 변환\n",
    "    (예: 맥락을 보강하거나 키워드 중심으로 재구성)\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"사용자의 질문을 검색에 유리하게 재작성해 주세요.\\n\"\n",
    "        \"원본 질문: {question}\\n\"\n",
    "        \"재작성된 질문:\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    rewritten = chain.invoke({\"question\": question})\n",
    "    return rewritten.strip()\n",
    "\n",
    "\n",
    "def milvus_search(question_embedding, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Milvus 검색, 검색 결과 리스트 반환\n",
    "    \"\"\"\n",
    "    res = client.search(\n",
    "        collection_name='kmu_llm',\n",
    "        data=[question_embedding],\n",
    "        limit=top_k,\n",
    "        output_fields=['id', 'text', 'meta']\n",
    "    )\n",
    "    if not res or len(res[0]) == 0:\n",
    "        return []\n",
    "    return res[0]  # HybridHits 리스트처럼 동작\n",
    "\n",
    "\n",
    "def rank_results(results) -> list:\n",
    "    \"\"\"\n",
    "    Re-ranking: 검색된 결과를 LLM을 이용해 재정렬\n",
    "    여기서는 간단히 LLM에게 순위를 다시 매기도록 요청\n",
    "    \"\"\"\n",
    "    # 결과를 텍스트와 ID 리스트로 만들기\n",
    "    candidates = []\n",
    "    for hit in results:\n",
    "        ent = hit.entity\n",
    "        meta = getattr(ent, \"meta\", {})\n",
    "        text = getattr(ent, \"text\", \"\")\n",
    "        source = meta.get(\"source\", \"\")\n",
    "        candidates.append({\"text\": text, \"source\": source})\n",
    "\n",
    "    # LLM에게 순위 매기기 요청\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"다음 문서들을 질문과 관련한 순서대로 재정렬하세요.\\n\\n\"\n",
    "        \"문서들:\\n\"\n",
    "        \"{docs}\\n\\n\"\n",
    "        \"재정렬된 순서의 인덱스들을 쉼표로 나열해서 주세요 (예: 2,1,3):\"\n",
    "    )\n",
    "    docs_str = \"\\n\\n\".join(f\"{i+1}. ({c['source']}) {c['text'][:200]}\" for i, c in enumerate(candidates))\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    order = chain.invoke({\"docs\": docs_str}).strip()\n",
    "    # order 예: \"2,1,3\"\n",
    "    try:\n",
    "        idxs = [int(x) - 1 for x in order.split(\",\")] \n",
    "    except:\n",
    "        # 실패 시 원래 순서 유지\n",
    "        idxs = list(range(len(candidates)))\n",
    "    reordered = [candidates[i] for i in idxs if 0 <= i < len(candidates)]\n",
    "    return reordered\n",
    "\n",
    "\n",
    "def compress_context(ranked_docs: list, max_total_chars: int = 1500) -> str:\n",
    "    \"\"\"\n",
    "    컨텍스트 압축: 여러 문서를 하나의 문자열로 합치되, 길이 제한 고려\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    total = 0\n",
    "    for doc in ranked_docs:\n",
    "        text = doc[\"text\"]\n",
    "        src = doc[\"source\"]\n",
    "        entry = f\"[{src}]\\n{text}\\n\"\n",
    "        l = len(entry)\n",
    "        if total + l > max_total_chars:\n",
    "            break\n",
    "        parts.append(entry)\n",
    "        total += l\n",
    "    return \"\\n---\\n\".join(parts)\n",
    "\n",
    "\n",
    "def verify_answer(question: str, answer: str, context: str) -> bool:\n",
    "    \"\"\"\n",
    "    Self-verification: LLM에게 답변이 컨텍스트에 근거했는지 확인 요청\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"아래는 질문, 답변, 그리고 문맥입니다.\\n\"\n",
    "        \"답변이 문맥에 근거한 것인지 검증해 주세요.\\n\"\n",
    "        \"만약 근거가 없다면 'No'를, 근거가 있으면 'Yes'를 출력하세요.\\n\\n\"\n",
    "        \"질문: {question}\\n\"\n",
    "        \"답변: {answer}\\n\"\n",
    "        \"문맥:\\n{context}\\n\\n\"\n",
    "        \"검증 결과:\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    res = chain.invoke({\"question\": question, \"answer\": answer, \"context\": context}).strip().lower()\n",
    "    return res.startswith(\"yes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056e26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def advanced_rag(question: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Advanced RAG 파이프라인\n",
    "    \"\"\"\n",
    "    # 1) 질문 재작성\n",
    "    rewritten = rewrite_question(question)\n",
    "\n",
    "    # 2) 재작성된 질문 임베딩\n",
    "    q_emb = embedding_model.embed_query(rewritten)\n",
    "\n",
    "    # 3) Milvus 검색\n",
    "    results = milvus_search(q_emb, top_k=top_k)\n",
    "\n",
    "    # 4) Re-ranking\n",
    "    ranked = rank_results(results)\n",
    "\n",
    "    # 5) 컨텍스트 압축\n",
    "    context = compress_context(ranked)\n",
    "\n",
    "    # 6) 답변 생성\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"다음 문맥을 참고하여 질문에 답하세요.\\n\\n\"\n",
    "        \"질문: {question}\\n\"\n",
    "        \"문맥:\\n{context}\\n\\n\"\n",
    "        \"답변:\"\n",
    "    )\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "    # 7) 검증\n",
    "    ok = verify_answer(question, answer, context)\n",
    "    if not ok:\n",
    "        return \"죄송합니다. 제공된 문서로는 충분한 정보를 찾을 수 없습니다.\"\n",
    "\n",
    "    # 8) 최종 반환 (출처 포함)\n",
    "    # 출처: ranked 문서의 source 리스트\n",
    "    sources = [doc[\"source\"] for doc in ranked]\n",
    "    unique_sources = list(dict.fromkeys(sources))\n",
    "    return f\"{answer}\\n\\n출처: {', '.join(unique_sources)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b523fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 경영정보학과 취득 가능 자격증 종류는 무엇인가요\n",
      "답변: 경영정보학과에서 취득 가능한 자격증 종류는 다음과 같습니다:\n",
      "\n",
      "1. 정보처리기사\n",
      "2. 빅데이터분석기사\n",
      "3. SQL개발자\n",
      "4. 데이터분석준전문가\n",
      "5. 경영정보시각화능력\n",
      "\n",
      "이 자격증을 제출하면 졸업논문이 면제됩니다.\n",
      "\n",
      "출처: /Users/n-jison/Desktop/jaeig/kmu-agent-course/AI_Agent/retrieve/files/2025_bigdata_bluebook.pdf, /Users/n-jison/Desktop/jaeig/kmu-agent-course/AI_Agent/retrieve/files/2025_mis_bluebook.pdf\n"
     ]
    }
   ],
   "source": [
    "question = \"경영정보학과 취득 가능 자격증 종류는 무엇인가요\"\n",
    "res = advanced_rag(question, top_k=5)\n",
    "print(\"질문:\", question)\n",
    "print(\"답변:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f13619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
