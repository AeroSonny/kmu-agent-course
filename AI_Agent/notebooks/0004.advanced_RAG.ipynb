{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a79cf6c",
   "metadata": {},
   "source": [
    "# ğŸš€ Advanced RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "## ğŸ“˜ ê°œìš”\n",
    "**Advanced RAG**ëŠ” ê¸°ë³¸ RAGë³´ë‹¤ í•œ ë‹¨ê³„ ë°œì „ëœ êµ¬ì¡°ë¡œ,  \n",
    "ë‹¨ìˆœíˆ â€œê²€ìƒ‰ â†’ ë¬¸ë§¥ ì‚½ì… â†’ ë‹µë³€ ìƒì„±â€ì˜ íë¦„ì„ ë„˜ì–´  \n",
    "**ì •í™•ë„**, **íš¨ìœ¨ì„±**, **í™•ì¥ì„±**ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ê¸°ìˆ ë“¤ì´ ê²°í•©ëœ í˜•íƒœì…ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© ê¸°ë³¸ RAG ë³µìŠµ\n",
    "\n",
    "### ğŸ”¹ ê¸°ë³¸ êµ¬ì¡°\n",
    "1. **Query Embedding**: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜  \n",
    "2. **Vector Search**: ë²¡í„° DBì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰  \n",
    "3. **Context Injection**: ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€  \n",
    "4. **LLM Generation**: LLMì´ ë‹µë³€ ìƒì„±\n",
    "\n",
    "### âš ï¸ í•œê³„ì \n",
    "- ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ì§„ì§œ ê´€ë ¨ ì •ë³´ë§Œ ì„ ë³„í•˜ê¸° ì–´ë ¤ì›€  \n",
    "- ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ëª¨í˜¸í•  ê²½ìš° ì ì ˆí•œ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•ŠìŒ  \n",
    "- LLMì˜ ë‹µë³€ì´ ì‹¤ì œ ê·¼ê±° ë¬¸ì„œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ (hallucination)  \n",
    "- ê¸´ ë¬¸ì„œë‚˜ ë‹¤ì¤‘ hop ì§ˆì˜ì— ëŒ€ì‘í•˜ê¸° ì–´ë ¤ì›€\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Advanced RAGì˜ ì£¼ìš” êµ¬ì„±ìš”ì†Œ\n",
    "\n",
    "### 1ï¸âƒ£ Query Rewriting (ì§ˆë¬¸ ê°œì„ )\n",
    "- LLMì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ **ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ì¬ì‘ì„±**\n",
    "- ì˜ˆì‹œ  \n",
    "  - ì‚¬ìš©ì: â€œê·¸ íšŒì‚¬ ë§¤ì¶œ ì–´ë• ì–´?â€  \n",
    "  - ë³€í™˜: â€œ2023ë…„ ì‚¼ì„±ì „ìì˜ ì—°ê°„ ë§¤ì¶œì•¡â€\n",
    "- LangChain ì˜ˆì‹œ: `MultiQueryRetriever`, `ContextualCompressionRetriever`\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Re-ranking (ì¬ì •ë ¬)\n",
    "- ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLM ë˜ëŠ” Cross-Encoderë¡œ ì¬í‰ê°€í•˜ì—¬  \n",
    "  **ê°€ì¥ ê´€ë ¨ë„ê°€ ë†’ì€ ë¬¸ì„œë§Œ ë‚¨ê¹€**\n",
    "- ì˜ˆì‹œ:  \n",
    "  - ìƒìœ„ 20ê°œ ë¬¸ì„œ â†’ LLM ê¸°ë°˜ ì¬í‰ê°€ â†’ ìƒìœ„ 5ê°œ ì‚¬ìš©\n",
    "- ë„êµ¬ ì˜ˆì‹œ: `bge-reranker`, `Cohere Rerank API`, LangChain `ReRanker`\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Context Compression / Summarization\n",
    "- ë„ˆë¬´ ê¸´ ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ LLMì— ë„£ì§€ ì•Šê³   \n",
    "  **ìš”ì•½(summarization)** ë˜ëŠ” **í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ(extractive compression)**  \n",
    "- LangChain: `ContextualCompressionRetriever`, `LLMChainExtractor`\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ Multi-hop Retrieval\n",
    "- í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë‚˜ëˆ„ì–´ **ë‹¨ê³„ì  ê²€ìƒ‰** ìˆ˜í–‰  \n",
    "- ì˜ˆì‹œ:  \n",
    "  1. â€œAI ë°˜ë„ì²´ ì£¼ìš” ê¸°ì—…ì€?â€  \n",
    "  2. â€œNVIDIAì˜ ë§¤ì¶œ êµ¬ì¡°ëŠ”?â€\n",
    "- ëŒ€í‘œ ê¸°ë²•: **Self-Ask**, **ReAct**, **Chain-of-Thought RAG**\n",
    "\n",
    "---\n",
    "\n",
    "### 5ï¸âƒ£ Hybrid Retrieval (í˜¼í•© ê²€ìƒ‰)\n",
    "- **ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ + ë©”íƒ€ë°ì´í„° í•„í„°ë§**ì„ ê²°í•©  \n",
    "- ì˜ˆì‹œ:  \n",
    "  `vector + keyword + date filter`\n",
    "- ì¥ì : ì˜ë¯¸ì  ìœ ì‚¬ì„±ê³¼ ì‹œê¸°ì  ì œì•½ì„ í•¨ê»˜ ë°˜ì˜  \n",
    "- Milvus: `hybrid_search()` ê¸°ëŠ¥ í™œìš© ê°€ëŠ¥\n",
    "\n",
    "---\n",
    "\n",
    "### 6ï¸âƒ£ Structured Retrieval / Graph RAG\n",
    "- ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ **ê·¸ë˜í”„(Neo4j, ArangoDB ë“±)** ë¡œ ì €ì¥  \n",
    "- LLMì´ ê·¸ë˜í”„ íƒìƒ‰ì„ í†µí•´ ì˜ë¯¸ì  ì—°ê²°ì„±ì„ ì°¾ì•„ëƒ„  \n",
    "- ì˜ˆì‹œ:  \n",
    "  - ë…¼ë¬¸ ì¸ìš© ê´€ê³„, FAQ ì—°ê²° êµ¬ì¡° ë“±  \n",
    "- ì¥ì : ë…¼ë¦¬ì  reasoningê³¼ ê·¼ê±° ì¶”ì ì— ê°•í•¨\n",
    "\n",
    "---\n",
    "\n",
    "### 7ï¸âƒ£ Response Verification & Grounding\n",
    "- LLMì´ ìƒì„±í•œ ë‹µë³€ì„ **ë‹¤ì‹œ ê²€ì¦**í•˜ì—¬ ì‹¤ì œ ê·¼ê±° ë¬¸ì„œì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸  \n",
    "- ë°©ë²•  \n",
    "  - LLMì´ ìŠ¤ìŠ¤ë¡œ â€œì´ ë‹µë³€ì´ ë¬¸ì„œ ë‚´ìš©ê³¼ ì¼ì¹˜í•©ë‹ˆê¹Œ?â€ ì¬ì§ˆë¬¸  \n",
    "  - ì¶œì²˜(Citation) ìë™ ì¶”ê°€  \n",
    "  - Self-RAG (Meta, 2024): ëª¨ë¸ì´ ìì²´ì ìœ¼ë¡œ â€œretrieve â†’ answer â†’ verify â†’ reviseâ€ ìˆ˜í–‰\n",
    "\n",
    "---\n",
    "\n",
    "### 8ï¸âƒ£ Caching / Memory / Feedback Loop\n",
    "- ê³¼ê±°ì˜ ì§ˆì˜ì‘ë‹µì„ ì €ì¥í•˜ì—¬ **ì†ë„ í–¥ìƒ ë° ì¬ì‚¬ìš©**  \n",
    "- ì˜¤ë‹µ ì‹œ LLMì´ í”¼ë“œë°±ì„ í•™ìŠµí•˜ë„ë¡ êµ¬ì„±  \n",
    "- ì˜ˆì‹œ: `LangChain Memory`, `VectorCache`, `User Feedback Loop`\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Advanced RAG íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ\n",
    "\n",
    "```text\n",
    "ì‚¬ìš©ì ì§ˆë¬¸\n",
    "   â†“\n",
    "**ì§ˆë¬¸ ì¬ì‘ì„± (Query Rewriter)** \n",
    "   â†“\n",
    "**Re-ranking (LLM ë˜ëŠ” Cross-Encoder)**\n",
    "   â†“\n",
    "Context Summarization (ì••ì¶•)\n",
    "   â†“\n",
    "LLM ë‹µë³€ ìƒì„±\n",
    "   â†“\n",
    "Answer Verification (Self-RAG)\n",
    "   â†“\n",
    "ê²°ê³¼ + ì¶œì²˜ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9050bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus Collections: ['kmu_llm']\n",
      "Collection stats: {'row_count': 206}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pymilvus import MilvusClient\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# â”€â”€ ì„¤ì • â”€â”€\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, openai_api_key=OPENAI_API_KEY)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Milvus ì—°ê²°\n",
    "db_dir = '/Users/n-jison/Desktop/jaeig/kmu-agent-course/AI_Agent/retrieve'\n",
    "db_name = 'kmu.db'\n",
    "db_url = os.path.join(db_dir, db_name)\n",
    "client = MilvusClient(db_url)\n",
    "\n",
    "# ì—°ê²° í™•ì¸\n",
    "collections = client.list_collections()\n",
    "print(\"Milvus Collections:\", collections)\n",
    "stats = client.get_collection_stats(collection_name='kmu_llm')\n",
    "print(\"Collection stats:\", stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc71b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advanced RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„\n",
    "def rewrite_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Query Rewriter: ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìœ ë¦¬í•œ í˜•íƒœë¡œ ë³€í™˜\n",
    "    (ì˜ˆ: ë§¥ë½ì„ ë³´ê°•í•˜ê±°ë‚˜ í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì¬êµ¬ì„±)\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìœ ë¦¬í•˜ê²Œ ì¬ì‘ì„±í•´ ì£¼ì„¸ìš”.\\n\"\n",
    "        \"ì›ë³¸ ì§ˆë¬¸: {question}\\n\"\n",
    "        \"ì¬ì‘ì„±ëœ ì§ˆë¬¸:\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    rewritten = chain.invoke({\"question\": question})\n",
    "    return rewritten.strip()\n",
    "\n",
    "\n",
    "def milvus_search(question_embedding, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Milvus ê²€ìƒ‰, ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    res = client.search(\n",
    "        collection_name='kmu_llm',\n",
    "        data=[question_embedding],\n",
    "        limit=top_k,\n",
    "        output_fields=['id', 'text', 'meta']\n",
    "    )\n",
    "    if not res or len(res[0]) == 0:\n",
    "        return []\n",
    "    return res[0]  # HybridHits ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ë™ì‘\n",
    "\n",
    "\n",
    "def rank_results(results) -> list:\n",
    "    \"\"\"\n",
    "    Re-ranking: ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ LLMì„ ì´ìš©í•´ ì¬ì •ë ¬\n",
    "    ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ LLMì—ê²Œ ìˆœìœ„ë¥¼ ë‹¤ì‹œ ë§¤ê¸°ë„ë¡ ìš”ì²­\n",
    "    \"\"\"\n",
    "    # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ì™€ ID ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°\n",
    "    candidates = []\n",
    "    for hit in results:\n",
    "        ent = hit.entity\n",
    "        meta = getattr(ent, \"meta\", {})\n",
    "        text = getattr(ent, \"text\", \"\")\n",
    "        source = meta.get(\"source\", \"\")\n",
    "        candidates.append({\"text\": text, \"source\": source})\n",
    "\n",
    "    # LLMì—ê²Œ ìˆœìœ„ ë§¤ê¸°ê¸° ìš”ì²­\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì§ˆë¬¸ê³¼ ê´€ë ¨í•œ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"ë¬¸ì„œë“¤:\\n\"\n",
    "        \"{docs}\\n\\n\"\n",
    "        \"ì¬ì •ë ¬ëœ ìˆœì„œì˜ ì¸ë±ìŠ¤ë“¤ì„ ì‰¼í‘œë¡œ ë‚˜ì—´í•´ì„œ ì£¼ì„¸ìš” (ì˜ˆ: 2,1,3):\"\n",
    "    )\n",
    "    docs_str = \"\\n\\n\".join(f\"{i+1}. ({c['source']}) {c['text'][:200]}\" for i, c in enumerate(candidates))\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    order = chain.invoke({\"docs\": docs_str}).strip()\n",
    "    # order ì˜ˆ: \"2,1,3\"\n",
    "    try:\n",
    "        idxs = [int(x) - 1 for x in order.split(\",\")] \n",
    "    except:\n",
    "        # ì‹¤íŒ¨ ì‹œ ì›ë˜ ìˆœì„œ ìœ ì§€\n",
    "        idxs = list(range(len(candidates)))\n",
    "    reordered = [candidates[i] for i in idxs if 0 <= i < len(candidates)]\n",
    "    return reordered\n",
    "\n",
    "\n",
    "def compress_context(ranked_docs: list, max_total_chars: int = 1500) -> str:\n",
    "    \"\"\"\n",
    "    ì»¨í…ìŠ¤íŠ¸ ì••ì¶•: ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ë˜, ê¸¸ì´ ì œí•œ ê³ ë ¤\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    total = 0\n",
    "    for doc in ranked_docs:\n",
    "        text = doc[\"text\"]\n",
    "        src = doc[\"source\"]\n",
    "        entry = f\"[{src}]\\n{text}\\n\"\n",
    "        l = len(entry)\n",
    "        if total + l > max_total_chars:\n",
    "            break\n",
    "        parts.append(entry)\n",
    "        total += l\n",
    "    return \"\\n---\\n\".join(parts)\n",
    "\n",
    "\n",
    "def verify_answer(question: str, answer: str, context: str) -> bool:\n",
    "    \"\"\"\n",
    "    Self-verification: LLMì—ê²Œ ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í–ˆëŠ”ì§€ í™•ì¸ ìš”ì²­\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"ì•„ë˜ëŠ” ì§ˆë¬¸, ë‹µë³€, ê·¸ë¦¬ê³  ë¬¸ë§¥ì…ë‹ˆë‹¤.\\n\"\n",
    "        \"ë‹µë³€ì´ ë¬¸ë§¥ì— ê·¼ê±°í•œ ê²ƒì¸ì§€ ê²€ì¦í•´ ì£¼ì„¸ìš”.\\n\"\n",
    "        \"ë§Œì•½ ê·¼ê±°ê°€ ì—†ë‹¤ë©´ 'No'ë¥¼, ê·¼ê±°ê°€ ìˆìœ¼ë©´ 'Yes'ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"ì§ˆë¬¸: {question}\\n\"\n",
    "        \"ë‹µë³€: {answer}\\n\"\n",
    "        \"ë¬¸ë§¥:\\n{context}\\n\\n\"\n",
    "        \"ê²€ì¦ ê²°ê³¼:\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    res = chain.invoke({\"question\": question, \"answer\": answer, \"context\": context}).strip().lower()\n",
    "    return res.startswith(\"yes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056e26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def advanced_rag(question: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Advanced RAG íŒŒì´í”„ë¼ì¸\n",
    "    \"\"\"\n",
    "    # 1) ì§ˆë¬¸ ì¬ì‘ì„±\n",
    "    rewritten = rewrite_question(question)\n",
    "\n",
    "    # 2) ì¬ì‘ì„±ëœ ì§ˆë¬¸ ì„ë² ë”©\n",
    "    q_emb = embedding_model.embed_query(rewritten)\n",
    "\n",
    "    # 3) Milvus ê²€ìƒ‰\n",
    "    results = milvus_search(q_emb, top_k=top_k)\n",
    "\n",
    "    # 4) Re-ranking\n",
    "    ranked = rank_results(results)\n",
    "\n",
    "    # 5) ì»¨í…ìŠ¤íŠ¸ ì••ì¶•\n",
    "    context = compress_context(ranked)\n",
    "\n",
    "    # 6) ë‹µë³€ ìƒì„±\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"ì§ˆë¬¸: {question}\\n\"\n",
    "        \"ë¬¸ë§¥:\\n{context}\\n\\n\"\n",
    "        \"ë‹µë³€:\"\n",
    "    )\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "    # 7) ê²€ì¦\n",
    "    ok = verify_answer(question, answer, context)\n",
    "    if not ok:\n",
    "        return \"ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¡œëŠ” ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    # 8) ìµœì¢… ë°˜í™˜ (ì¶œì²˜ í¬í•¨)\n",
    "    # ì¶œì²˜: ranked ë¬¸ì„œì˜ source ë¦¬ìŠ¤íŠ¸\n",
    "    sources = [doc[\"source\"] for doc in ranked]\n",
    "    unique_sources = list(dict.fromkeys(sources))\n",
    "    return f\"{answer}\\n\\nì¶œì²˜: {', '.join(unique_sources)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b523fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸: ê²½ì˜ì •ë³´í•™ê³¼ ì·¨ë“ ê°€ëŠ¥ ìê²©ì¦ ì¢…ë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”\n",
      "ë‹µë³€: ê²½ì˜ì •ë³´í•™ê³¼ì—ì„œ ì·¨ë“ ê°€ëŠ¥í•œ ìê²©ì¦ ì¢…ë¥˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. ì •ë³´ì²˜ë¦¬ê¸°ì‚¬\n",
      "2. ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬\n",
      "3. SQLê°œë°œì\n",
      "4. ë°ì´í„°ë¶„ì„ì¤€ì „ë¬¸ê°€\n",
      "5. ê²½ì˜ì •ë³´ì‹œê°í™”ëŠ¥ë ¥\n",
      "\n",
      "ì´ ìê²©ì¦ì„ ì œì¶œí•˜ë©´ ì¡¸ì—…ë…¼ë¬¸ì´ ë©´ì œë©ë‹ˆë‹¤.\n",
      "\n",
      "ì¶œì²˜: /Users/n-jison/Desktop/jaeig/kmu-agent-course/AI_Agent/retrieve/files/2025_bigdata_bluebook.pdf, /Users/n-jison/Desktop/jaeig/kmu-agent-course/AI_Agent/retrieve/files/2025_mis_bluebook.pdf\n"
     ]
    }
   ],
   "source": [
    "question = \"ê²½ì˜ì •ë³´í•™ê³¼ ì·¨ë“ ê°€ëŠ¥ ìê²©ì¦ ì¢…ë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”\"\n",
    "res = advanced_rag(question, top_k=5)\n",
    "print(\"ì§ˆë¬¸:\", question)\n",
    "print(\"ë‹µë³€:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f13619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
