{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bf42ce",
   "metadata": {},
   "source": [
    "## RAG 구축\n",
    "# 📚 RAG (Retrieval-Augmented Generation) 개념 정리\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ RAG란?\n",
    "\n",
    "- **RAG**는 **Retrieval-Augmented Generation**의 약어로, **생성형 AI(LLM)**와 **정보 검색(IR, 벡터 검색 등)**을 결합한 아키텍처입니다.  [oai_citation:0‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "- 일반 LLM이 내부 학습된 고정된 지식에만 의존하는 반면, RAG는 **외부 지식 저장소(문서, 데이터베이스 등)** 를 질의 시점에 참조하여 응답을 보강합니다.  [oai_citation:1‡위키백과](https://en.wikipedia.org/wiki/Retrieval-augmented_generation?utm_source=chatgpt.com)  \n",
    "- 이를 통해 LLM이 갖지 못한 최신 정보, 도메인별 문서 기반 지식 등을 활용할 수 있게 하며, 환각(hallucination)을 줄이고 응답의 정확성과 신뢰성을 높이는 효과가 있습니다.  [oai_citation:2‡NVIDIA Blog](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ RAG의 기본 구조 / 워크플로우\n",
    "\n",
    "RAG 시스템은 일반적으로 다음 4단계로 구성됩니다:\n",
    "\n",
    "1. **인덱싱 / 색인화(Indexing)**  \n",
    "   문서나 데이터 소스를 전처리하고 임베딩(벡터화)하여 벡터 저장소(vector database)에 저장합니다.  [oai_citation:3‡위키백과](https://en.wikipedia.org/wiki/Retrieval-augmented_generation?utm_source=chatgpt.com)  \n",
    "\n",
    "2. **리트리벌(Retrieval)**  \n",
    "   사용자의 질의(Query)를 임베딩하여, 벡터 저장소에서 유사 문서들을 검색합니다.  [oai_citation:4‡위키백과](https://en.wikipedia.org/wiki/Retrieval-augmented_generation?utm_source=chatgpt.com)  \n",
    "\n",
    "3. **증강(Augmentation 또는 Context Injection)**  \n",
    "   검색된 문서(문맥)를 질의와 함께 프롬프트에 포함시켜 LLM이 참고할 수 있게 합니다.  [oai_citation:5‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "\n",
    "4. **생성(Generation)**  \n",
    "   LLM이 증강된 입력(질의 + 문맥)을 바탕으로 응답을 생성합니다.  [oai_citation:6‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "\n",
    "> 이 흐름은 “Retrieve → Augment → Generate” 또는 “검색 → 보강 → 생성” 패턴으로 간단히 요약되기도 합니다.  [oai_citation:7‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ RAG의 장점 및 활용 효과\n",
    "\n",
    "| 장점 | 설명 |\n",
    "|---|------|\n",
    "| **최신 정보 반영 가능** | 외부 지식 기반을 참조함으로써, 학습 이후 생긴 최신 사실이나 도메인 지식을 활용 가능 |\n",
    "| **환각 감소** | 응답 시 문맥을 기반으로 생성하므로, LLM이 아무 근거 없이 답을 만드는 오류를 줄일 수 있음  [oai_citation:8‡NVIDIA Blog](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?utm_source=chatgpt.com) |\n",
    "| **비용 효율성** | 모델을 자주 재학습할 필요 없이, 외부 지식 저장소만 업데이트하면 됨  [oai_citation:9‡위키백과](https://en.wikipedia.org/wiki/Retrieval-augmented_generation?utm_source=chatgpt.com) |\n",
    "| **출처 제시 가능성** | 검색된 문서를 응답과 함께 인용하거나 참조하게 하여 답변의 투명성과 검증 가능성을 제공 |\n",
    "| **도메인 특화 응답** | 일반적인 LLM보다는 특정 문서 기반 지식에 강한 응답 가능 |\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ 한계 및 도전 과제\n",
    "\n",
    "| 한계 / 리스크 | 설명 |\n",
    "|---|------|\n",
    "| **잘못된 문서 선정 가능성** | 검색된 문서가 실제 질문과 어울리지 않을 경우, 부적절한 문맥이 포함될 수 있음 |\n",
    "| **문맥 길이 한계** | 너무 많은 문서를 포함하면 프롬프트 길이 제한(token limit)에 걸릴 수 있고, 핵심 문맥 손실 우려 |\n",
    "| **응답 오류 여전** | 문맥이 정확해도 LLM이 잘못 해석하거나 범위 밖으로 벗어난 응답을 만들 수 있음 |\n",
    "| **검색 비용 / 성능 부담** | 대규모 벡터 검색 시스템 운영 비용, 인덱스 구축 및 유지 비용이 발생 |\n",
    "| **데이터 일관성 / 동기화 문제** | 문서가 자주 바뀔 경우 인덱스 갱신 및 버전 관리가 필요 |\n",
    "| **보안 / 접근 통제 위험** | 내부 문서나 민감 데이터를 중앙 벡터 저장소에 모을 경우 보안 이슈 발생 가능 |\n",
    "\n",
    "또한 일부 기업에서는 **“RAG는 구식이고 대신 에이전트 기반 AI 구조로 전환 중”**이라는 주장도 나오고 있습니다.  [oai_citation:10‡TechRadar](https://www.techradar.com/pro/rag-is-dead-why-enterprises-are-shifting-to-agent-based-ai-architectures?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ RAG 구현 시 고려 요소 / 팁\n",
    "\n",
    "- **검색 정확도 강화**  \n",
    " 단순 벡터 검색 외에 **하이브리드 검색** (벡터 + 키워드 기반 검색 혼합) 기법 사용  \n",
    " 또한 리랭커(reranker)를 두어 검색된 문서 우선순위를 재정렬  \n",
    "\n",
    "- **Chunking 전략**  \n",
    " 문서를 어떻게 나눌지 (고정 크기 / 문장 기준 / 구조 기반) 설계해야 검색 품질이 향상됨  \n",
    "\n",
    "- **임베딩 모델 선택**  \n",
    " 도메인 특화 모델을 쓰거나 미세 조정한 모델이 더 좋은 검색 품질을 줄 수 있음  \n",
    "\n",
    "- **컨텍스트 관리 / 프롬프트 설계**  \n",
    " 문맥 과부하를 방지하고, 중요 문맥만 제공하는 방식 설계  \n",
    " “컨텍스트 외 응답 자제” 규칙, 요약 + 인용 방식 병행  \n",
    "\n",
    "- **인덱스 및 저장소 튜닝**  \n",
    " 벡터 저장소(예: FAISS, Milvus 등)의 인덱스 파라미터, 검색 파라미터(nprobe 등)를 최적화  \n",
    "\n",
    "- **응답 검증 및 평가 지표**  \n",
    " 정확도, 재현율, F1, 응답 속도, 사용자 만족도 등으로 모니터링  \n",
    "\n",
    "- **지속적 업데이트 전략**  \n",
    " 새 문서 추가, 삭제, 수정이 생길 때 인덱스 동기화 방식 설계  \n",
    "\n",
    "---\n",
    "\n",
    "## 6️⃣ LangChain에서의 RAG\n",
    "\n",
    "LangChain은 RAG를 쉽게 구현할 수 있도록 여러 추상화 계층을 제공합니다:  \n",
    "- **Vectorstores + Retrievers** 모듈로 문서 검색 기능 제공  \n",
    "- **Chain / LCEL / Runnable** 구성 요소로 검색 결과 + 질의를 조합해 응답 생성  \n",
    "- **LangSmith**로 검색 + 생성 흐름을 추적하고 모니터링 가능  [oai_citation:11‡LangChain](https://python.langchain.com/docs/tutorials/qa_chat_history/?utm_source=chatgpt.com)  \n",
    "- LangChain 공식 문서는 RAG 개념을 중심으로 **Retrieval → Augmentation → Generation** 흐름을 설명합니다  [oai_citation:12‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 7️⃣ 최근 연구 동향 & 발전\n",
    "\n",
    "- **Knowledge-Oriented RAG**: 외부 지식 그래프나 구조화된 지식을 활용해 검색 + 생성의 정밀성을 높이는 연구가 진행 중입니다.  [oai_citation:13‡arXiv](https://arxiv.org/abs/2503.10677?utm_source=chatgpt.com)  \n",
    "- **Prompt-RAG**: 벡터 임베딩 없이 프롬프트 기반 검색/증강 방식을 사용하는 연구 (특히 도메인 특화 분야)도 제안됨  [oai_citation:14‡arXiv](https://arxiv.org/abs/2401.11246?utm_source=chatgpt.com)  \n",
    "- **OG-RAG (Ontology-Grounded RAG)**: 도메인 온톨로지를 기반으로 검색된 문서를 구조적으로 선택/결합하는 방식 제안됨  [oai_citation:15‡arXiv](https://arxiv.org/abs/2412.15235?utm_source=chatgpt.com)  \n",
    "- RAG 방법론을 **사전 검색 단계, 후처리 단계**로 세분화하고 체계화한 최근 리뷰 논문들도 발표되고 있습니다.  [oai_citation:16‡arXiv](https://arxiv.org/abs/2404.10981?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "필요하시면 이 Markdown을 교육용 슬라이드용 요약본으로 정리해드릴까요? 또는 LangChain 기반 실전 RAG 예제 코드도 같이 정리해드릴까요?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77485469",
   "metadata": {},
   "source": [
    "#### RAG (Retrieval Augmented Generation) - 실행해보기\n",
    "\n",
    "- 1. preprocessing (데이터 전처리) (수행완료 - milvus에 데이터 적재 완료) -> kmu_llm 컬렉션에 데이터 적재됨\n",
    "- 2. retrieval (데이터 검색) +  chatbot (챗봇)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fbf46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n-jison/Desktop/jaeig/kmu-agent-course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    " ## 필요 라이브러리 임포트 및 환경 변수 설정\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "# Milvus (pymilvus 2.4+)\n",
    "from pymilvus import MilvusClient\n",
    "from langchain.vectorstores import Milvus\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a5cbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/m4sc4mrs0md76r66w4tbnv_40000gn/T/ipykernel_25673/3396186098.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(model = 'text-embedding-3-small',\n"
     ]
    }
   ],
   "source": [
    "## LLM 모델 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\",\n",
    "                temperature=0.1, \n",
    "                openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "## 임베딩 설정\n",
    "embedding_model = OpenAIEmbeddings(model = 'text-embedding-3-small', \n",
    "                                openai_api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec5cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MilvusDB at /Users/n-jison/Desktop/jaeig/kmu-agent-course/AI_Agent/retrieve/kmu.db, Collections: ['kmu_llm']\n",
      "Collection stats: {'row_count': 206}\n"
     ]
    }
   ],
   "source": [
    "## 1. MilvusDB 연결 설정\n",
    "db_dir = '/Users/n-jison/Desktop/jaeig/kmu-agent-course/AI_Agent/retrieve/'  # DB 디렉토리 경로\n",
    "db_name = 'kmu.db' # DB 파일 이름\n",
    "db_url = os.path.join(db_dir, db_name) # 전체 DB 경로\n",
    "client = MilvusClient(db_url) # Milvus 클라이언트 생성\n",
    "\n",
    "## 연결 확인 \n",
    "stats = client.list_collections()\n",
    "if stats:  #\n",
    "    print(f\"Connected to MilvusDB at {db_url}, Collections: {stats}\")\n",
    "else:\n",
    "    print(f\"Failed to connect to MilvusDB at {db_url}\")\n",
    "\n",
    "## 데이터 개수 확인\n",
    "stats = client.get_collection_stats(collection_name='kmu_llm')\n",
    "print(f\"Collection stats: {stats}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. vectorDB 검색 & langchain 구성 \n",
    "def question_embed(question:str) -> list:\n",
    "    return embedding_model.embed_query(question)\n",
    "    \n",
    "\n",
    "def rag_query(client: MilvusClient,\n",
    "              question: str,\n",
    "              top_k: int = 7) -> str:\n",
    "    # 1) 질문 임베딩\n",
    "    query_emb = embedding_model.embed_query(question)\n",
    "\n",
    "    # 2) 벡터 검색\n",
    "    res = client.search(\n",
    "        collection_name='kmu_llm',\n",
    "        data=[query_emb],                 # 최신 pymilvus에서는 data 사용\n",
    "        limit=top_k,\n",
    "        output_fields=['id', 'text', 'meta']\n",
    "    )\n",
    "    # print(f\"Search results: {res}\")\n",
    "    # 3) 실제 매치 리스트 추출\n",
    "    if not res or len(res[0]) == 0:\n",
    "        return \"관련 문서가 검색되지 않았습니다.\"\n",
    "\n",
    "    # 4) 컨텍스트 문자열 구성 (출처 포함, 길이 제한)\n",
    "    context_parts = []\n",
    "    for i, m in enumerate(res[0]):\n",
    "        # print(f\"유사도: {m.distance}\")\n",
    "        ent = m.entity\n",
    "        meta = ent.meta \n",
    "        text = ent.text\n",
    "\n",
    "        context_parts.append(f\"{i+1}. {text}\")\n",
    "\n",
    "    context = \"\\n\".join(context_parts)\n",
    "    # print(context)\n",
    "    return context, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fceb701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 경영정보학과는 어떤 학과인가요?\n",
      "답변: ('경영정보학과는 비즈니스와 정보기술이 결합된 분야로, 성공적인 비즈니스를 위해 필요한 정보와 의사결정의 중요성을 이해하고 실무에 적용하는 학과입니다. 이 학과에서는 경영이론과 함께 빅데이터, 프로그래밍, 소프트웨어, 정보시스템 등 다양한 기술과 기법을 배우며, 이를 통해 경영에 필요한 통찰력을 기르는 것이 목표입니다. 즉, 경영과 정보기술을 융합하여 기업의 문제를 해결하고 혁신을 이끄는 전문가를 양성하는 학과라고 할 수 있습니다.',)\n"
     ]
    }
   ],
   "source": [
    "## 3. 챗봇 구성\n",
    "CHATBOT_PROMPT = \"\"\"\n",
    "## 지시사항\n",
    "당신은 사용자에게 도움을 주는 챗봇입니다. \n",
    "사용자가 하는 질의에 대해, 검색된 문서를 바탕으로 답변하세요 \n",
    "\n",
    "## 제약사항\n",
    "- 검색된 문서의 내용을 바탕으로, 사용자가 이해하기 쉽게 답변하세요\n",
    "\n",
    "## 입력\n",
    "- 질문: {question}\n",
    "- 검색된 문서:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(CHATBOT_PROMPT)\n",
    "\n",
    "\n",
    "def chat_with_rag(llm,\n",
    "                prompt:ChatPromptTemplate,\n",
    "                question:str,\n",
    "                context:str) -> str:\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({'question': question, 'context': context})\n",
    "    # print(context)\n",
    "    return response\n",
    "\n",
    "## 4. RAG 챗봇 실행\n",
    "def rag_chatbot(client:MilvusClient,\n",
    "                llm,\n",
    "                prompt:ChatPromptTemplate,\n",
    "                question:str,\n",
    "                top_k:int=3) -> str:\n",
    "    context = rag_query(client, question, top_k)\n",
    "    answer = chat_with_rag(llm, prompt, question, context)\n",
    "    return answer, \n",
    "\n",
    "\n",
    "## 예시 질의\n",
    "question = \"경영정보학과는 어떤 학과인가요?\"\n",
    "response = rag_chatbot(client, llm, prompt, question, top_k=5)\n",
    "\n",
    "print(\"질문:\", question)\n",
    "print(\"답변:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2a14a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture2 (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
