{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bf42ce",
   "metadata": {},
   "source": [
    "## RAG 구축\n",
    "# 📚 RAG (Retrieval-Augmented Generation) 개념 정리\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ RAG란?\n",
    "\n",
    "- **RAG**는 **Retrieval-Augmented Generation**의 약어로, **생성형 AI(LLM)**와 **정보 검색(IR, 벡터 검색 등)**을 결합한 아키텍처입니다.  [oai_citation:0‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "- 일반 LLM이 내부 학습된 고정된 지식에만 의존하는 반면, RAG는 **외부 지식 저장소(문서, 데이터베이스 등)** 를 질의 시점에 참조하여 응답을 보강합니다.  [oai_citation:1‡위키백과](https://en.wikipedia.org/wiki/Retrieval-augmented_generation?utm_source=chatgpt.com)  \n",
    "- 이를 통해 LLM이 갖지 못한 최신 정보, 도메인별 문서 기반 지식 등을 활용할 수 있게 하며, 환각(hallucination)을 줄이고 응답의 정확성과 신뢰성을 높이는 효과가 있습니다.  [oai_citation:2‡NVIDIA Blog](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ RAG의 기본 구조 / 워크플로우\n",
    "\n",
    "RAG 시스템은 일반적으로 다음 4단계로 구성됩니다:\n",
    "\n",
    "1. **인덱싱 / 색인화(Indexing)**  \n",
    "   문서나 데이터 소스를 전처리하고 임베딩(벡터화)하여 벡터 저장소(vector database)에 저장합니다.  [oai_citation:3‡위키백과](https://en.wikipedia.org/wiki/Retrieval-augmented_generation?utm_source=chatgpt.com)  \n",
    "\n",
    "2. **리트리벌(Retrieval)**  \n",
    "   사용자의 질의(Query)를 임베딩하여, 벡터 저장소에서 유사 문서들을 검색합니다.  [oai_citation:4‡위키백과](https://en.wikipedia.org/wiki/Retrieval-augmented_generation?utm_source=chatgpt.com)  \n",
    "\n",
    "3. **증강(Augmentation 또는 Context Injection)**  \n",
    "   검색된 문서(문맥)를 질의와 함께 프롬프트에 포함시켜 LLM이 참고할 수 있게 합니다.  [oai_citation:5‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "\n",
    "4. **생성(Generation)**  \n",
    "   LLM이 증강된 입력(질의 + 문맥)을 바탕으로 응답을 생성합니다.  [oai_citation:6‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "\n",
    "> 이 흐름은 “Retrieve → Augment → Generate” 또는 “검색 → 보강 → 생성” 패턴으로 간단히 요약되기도 합니다.  [oai_citation:7‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ RAG의 장점 및 활용 효과\n",
    "\n",
    "| 장점 | 설명 |\n",
    "|---|------|\n",
    "| **최신 정보 반영 가능** | 외부 지식 기반을 참조함으로써, 학습 이후 생긴 최신 사실이나 도메인 지식을 활용 가능 |\n",
    "| **환각 감소** | 응답 시 문맥을 기반으로 생성하므로, LLM이 아무 근거 없이 답을 만드는 오류를 줄일 수 있음  [oai_citation:8‡NVIDIA Blog](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?utm_source=chatgpt.com) |\n",
    "| **비용 효율성** | 모델을 자주 재학습할 필요 없이, 외부 지식 저장소만 업데이트하면 됨  [oai_citation:9‡위키백과](https://en.wikipedia.org/wiki/Retrieval-augmented_generation?utm_source=chatgpt.com) |\n",
    "| **출처 제시 가능성** | 검색된 문서를 응답과 함께 인용하거나 참조하게 하여 답변의 투명성과 검증 가능성을 제공 |\n",
    "| **도메인 특화 응답** | 일반적인 LLM보다는 특정 문서 기반 지식에 강한 응답 가능 |\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ 한계 및 도전 과제\n",
    "\n",
    "| 한계 / 리스크 | 설명 |\n",
    "|---|------|\n",
    "| **잘못된 문서 선정 가능성** | 검색된 문서가 실제 질문과 어울리지 않을 경우, 부적절한 문맥이 포함될 수 있음 |\n",
    "| **문맥 길이 한계** | 너무 많은 문서를 포함하면 프롬프트 길이 제한(token limit)에 걸릴 수 있고, 핵심 문맥 손실 우려 |\n",
    "| **응답 오류 여전** | 문맥이 정확해도 LLM이 잘못 해석하거나 범위 밖으로 벗어난 응답을 만들 수 있음 |\n",
    "| **검색 비용 / 성능 부담** | 대규모 벡터 검색 시스템 운영 비용, 인덱스 구축 및 유지 비용이 발생 |\n",
    "| **데이터 일관성 / 동기화 문제** | 문서가 자주 바뀔 경우 인덱스 갱신 및 버전 관리가 필요 |\n",
    "| **보안 / 접근 통제 위험** | 내부 문서나 민감 데이터를 중앙 벡터 저장소에 모을 경우 보안 이슈 발생 가능 |\n",
    "\n",
    "또한 일부 기업에서는 **“RAG는 구식이고 대신 에이전트 기반 AI 구조로 전환 중”**이라는 주장도 나오고 있습니다.  [oai_citation:10‡TechRadar](https://www.techradar.com/pro/rag-is-dead-why-enterprises-are-shifting-to-agent-based-ai-architectures?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ RAG 구현 시 고려 요소 / 팁\n",
    "\n",
    "- **검색 정확도 강화**  \n",
    " 단순 벡터 검색 외에 **하이브리드 검색** (벡터 + 키워드 기반 검색 혼합) 기법 사용  \n",
    " 또한 리랭커(reranker)를 두어 검색된 문서 우선순위를 재정렬  \n",
    "\n",
    "- **Chunking 전략**  \n",
    " 문서를 어떻게 나눌지 (고정 크기 / 문장 기준 / 구조 기반) 설계해야 검색 품질이 향상됨  \n",
    "\n",
    "- **임베딩 모델 선택**  \n",
    " 도메인 특화 모델을 쓰거나 미세 조정한 모델이 더 좋은 검색 품질을 줄 수 있음  \n",
    "\n",
    "- **컨텍스트 관리 / 프롬프트 설계**  \n",
    " 문맥 과부하를 방지하고, 중요 문맥만 제공하는 방식 설계  \n",
    " “컨텍스트 외 응답 자제” 규칙, 요약 + 인용 방식 병행  \n",
    "\n",
    "- **인덱스 및 저장소 튜닝**  \n",
    " 벡터 저장소(예: FAISS, Milvus 등)의 인덱스 파라미터, 검색 파라미터(nprobe 등)를 최적화  \n",
    "\n",
    "- **응답 검증 및 평가 지표**  \n",
    " 정확도, 재현율, F1, 응답 속도, 사용자 만족도 등으로 모니터링  \n",
    "\n",
    "- **지속적 업데이트 전략**  \n",
    " 새 문서 추가, 삭제, 수정이 생길 때 인덱스 동기화 방식 설계  \n",
    "\n",
    "---\n",
    "\n",
    "## 6️⃣ LangChain에서의 RAG\n",
    "\n",
    "LangChain은 RAG를 쉽게 구현할 수 있도록 여러 추상화 계층을 제공합니다:  \n",
    "- **Vectorstores + Retrievers** 모듈로 문서 검색 기능 제공  \n",
    "- **Chain / LCEL / Runnable** 구성 요소로 검색 결과 + 질의를 조합해 응답 생성  \n",
    "- **LangSmith**로 검색 + 생성 흐름을 추적하고 모니터링 가능  [oai_citation:11‡LangChain](https://python.langchain.com/docs/tutorials/qa_chat_history/?utm_source=chatgpt.com)  \n",
    "- LangChain 공식 문서는 RAG 개념을 중심으로 **Retrieval → Augmentation → Generation** 흐름을 설명합니다  [oai_citation:12‡LangChain](https://python.langchain.com/docs/concepts/rag/?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 7️⃣ 최근 연구 동향 & 발전\n",
    "\n",
    "- **Knowledge-Oriented RAG**: 외부 지식 그래프나 구조화된 지식을 활용해 검색 + 생성의 정밀성을 높이는 연구가 진행 중입니다.  [oai_citation:13‡arXiv](https://arxiv.org/abs/2503.10677?utm_source=chatgpt.com)  \n",
    "- **Prompt-RAG**: 벡터 임베딩 없이 프롬프트 기반 검색/증강 방식을 사용하는 연구 (특히 도메인 특화 분야)도 제안됨  [oai_citation:14‡arXiv](https://arxiv.org/abs/2401.11246?utm_source=chatgpt.com)  \n",
    "- **OG-RAG (Ontology-Grounded RAG)**: 도메인 온톨로지를 기반으로 검색된 문서를 구조적으로 선택/결합하는 방식 제안됨  [oai_citation:15‡arXiv](https://arxiv.org/abs/2412.15235?utm_source=chatgpt.com)  \n",
    "- RAG 방법론을 **사전 검색 단계, 후처리 단계**로 세분화하고 체계화한 최근 리뷰 논문들도 발표되고 있습니다.  [oai_citation:16‡arXiv](https://arxiv.org/abs/2404.10981?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "필요하시면 이 Markdown을 교육용 슬라이드용 요약본으로 정리해드릴까요? 또는 LangChain 기반 실전 RAG 예제 코드도 같이 정리해드릴까요?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fbf46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY is None:\n",
    "    print('CAUTION: OPENAI_API_KEY is not set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77485469",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing.ipynb에서 있었 던 과정을 통해, 전체 데이터를 적재하기\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
