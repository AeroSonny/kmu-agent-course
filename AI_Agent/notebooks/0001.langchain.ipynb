{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed12191",
   "metadata": {},
   "source": [
    "# 🧩 LangChain (랭체인) 요약 자료\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ LangChain이란?\n",
    "\n",
    "LangChain은 **대규모 언어 모델(LLM, Large Language Model)** 을 활용하여  \n",
    "지능형 애플리케이션을 개발할 수 있도록 지원하는 **오픈소스 프레임워크**입니다.\n",
    "\n",
    "- 단순한 텍스트 생성에서 벗어나, 외부 데이터 연동, 워크플로우 구성, 상태 관리 등을 가능하게 합니다.  \n",
    "- 즉, “문서 읽기 → 요약 → 질의응답 → 결과 저장” 같은 과정을 하나의 **체인(chain)** 으로 자동화할 수 있습니다.  \n",
    "- LLM을 단독으로 사용하는 것보다 훨씬 복잡한 언어 기반 시스템 구축이 가능합니다.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ LangChain의 주요 구성 요소\n",
    "\n",
    "LangChain은 다음과 같은 **모듈형 구조**를 통해 다양한 기능을 제공합니다.\n",
    "\n",
    "| 구성 요소 | 설명 |\n",
    "|------------|------|\n",
    "| **Model I/O** | LLM에 입력할 프롬프트를 구성하고, 응답을 파싱하거나 제어합니다. |\n",
    "| **Data Connection** | 문서 로더, 변환기, 검색기 등을 통해 외부 데이터와 모델을 연결합니다. |\n",
    "| **Chains** | 여러 단계를 순차적으로 실행하여 복잡한 태스크를 수행합니다. |\n",
    "| **Agents** | 상황에 따라 적절한 도구나 체인을 동적으로 선택하여 실행합니다. |\n",
    "| **Memory** | 대화나 상태 정보를 저장하고 문맥(Context)을 유지합니다. |\n",
    "| **Callbacks** | 실행 중간 단계에서 로깅, 모니터링, 스트리밍 처리를 지원합니다. |\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ 개발 → 테스트 → 배포 흐름\n",
    "\n",
    "LangChain은 단순한 개발 도구를 넘어서, **품질 관리와 배포까지** 포함하는 생태계를 제공합니다.\n",
    "\n",
    "1. **개발 단계**  \n",
    "   - 체인, 에이전트, 메모리 등 모듈을 이용해 로직을 구성  \n",
    "   - 여러 LLM(OpenAI, Anthropic 등)과 외부 데이터 통합\n",
    "\n",
    "2. **테스트 / 품질 관리**  \n",
    "   - **LangSmith**를 통해 응답 품질과 편향성을 분석하고 디버깅할 수 있습니다.\n",
    "\n",
    "3. **배포 단계**  \n",
    "   - **LangServe**: 체인을 REST API 형태로 배포  \n",
    "   - **LangGraph / LangGraph Cloud**: 상태 기반(Stateful) 애플리케이션 모델링 및 실행  \n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ 주요 기능 요약\n",
    "\n",
    "- **데이터 통합:** PDF, 웹페이지, DB 등 다양한 자료를 LLM과 연결  \n",
    "- **체인 기반 워크플로우:** 복합 작업을 자동으로 연결 수행  \n",
    "- **에이전트 기반 자동화:** 명령에 따라 적절한 툴 선택 및 실행  \n",
    "- **메모리 관리:** 대화 맥락과 상태를 저장해 문맥 유지  \n",
    "- **임베딩(Embedding):** 텍스트를 벡터화하여 유사도 검색 가능  \n",
    "- **콜백 기능:** 로깅, 오류 처리, 스트리밍 등 모니터링 가능  \n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ 장점 및 한계 (비판적 시각)\n",
    "\n",
    "### ✔️ 장점\n",
    "- **확장성**: 다양한 LLM과 데이터 소스를 손쉽게 교체 및 조합 가능  \n",
    "- **생산성**: 반복 구현을 줄여 개발 속도 향상  \n",
    "- **테스트 및 배포 생태계 완비**  \n",
    "- **오픈소스 기반 커뮤니티 활성화**\n",
    "\n",
    "### ⚠️ 한계\n",
    "- **복잡성 증가**: 체인이나 에이전트 구성이 복잡할수록 성능 저하 가능  \n",
    "- **학습 곡선 존재**: 초보자에게는 진입 장벽이 높음  \n",
    "- **LLM 한계 유지**: 환각(Hallucination)이나 부정확한 응답 문제는 완전히 해결되지 않음  \n",
    "- **특정 도메인 적합성 한정**: 범용보다는 목적형 설계 필요  \n",
    "\n",
    "---\n",
    "\n",
    "## 6️⃣ 실제 적용 사례\n",
    "\n",
    "| 기업 | 적용 분야 | 설명 |\n",
    "|------|------------|------|\n",
    "| **Morningstar** | 금융 분석 | 보고서 요약 및 투자 인사이트 생성 |\n",
    "| **NCL (노르웨이지안 크루즈라인)** | 여행 추천 | 사용자 취향 기반 여행 일정 제안 |\n",
    "| **Elastic** | 보안 자동화 | 경고 요약, 로그 분석, 쿼리 자동 생성 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 최종 요약\n",
    "\n",
    "LangChain은 **LLM을 실제 서비스 수준으로 확장하는 핵심 프레임워크**입니다.  \n",
    "데이터, 메모리, 에이전트, 체인을 조합하여 복잡한 인공지능 응용을 구현할 수 있습니다.  \n",
    "다만 **복잡성·성능·비용 측면에서의 한계**를 인식하고 신중히 적용해야 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "**📚 전체 출처 목록**\n",
    "1. [삼성SDS - LangChain이란 무엇인가?](https://www.samsungsds.com/kr/insights/what-is-langchain.html)  \n",
    "2. [삼성SDS - LangChain 개념 및 구조](https://www.samsungsds.com/kr/insights/the-concept-of-langchain.html)  \n",
    "3. [삼성SDS - LangChain 프레임워크 소개](https://www.samsungsds.com/kr/insights/langchain-framework.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344a12e",
   "metadata": {},
   "source": [
    "# 🧩 LLM API 대신 LangChain을 사용하는 이유\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 1. LLM API의 한계\n",
    "\n",
    "일반적인 LLM API(OpenAI, Claude, Gemini 등)는 다음과 같은 구조를 가집니다:\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"요약해줘\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "즉, **입력(prompt) → 출력(text)** 의 단순한 1회성 구조입니다.\n",
    "\n",
    "> ⚠️ 문제점  \n",
    "> - 외부 데이터 접근 불가  \n",
    "> - 대화 문맥(state) 유지 불가  \n",
    "> - 복잡한 작업(요약 → 분석 → 응답) 불가능  \n",
    "> - 품질 모니터링 및 관리 기능 부재  \n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 2. LangChain의 핵심 역할\n",
    "\n",
    "LangChain은 LLM을 **앱 로직과 통합**하여 “지능형 워크플로우”로 확장하는 프레임워크입니다.  \n",
    "즉, LLM + 데이터 + 논리 흐름을 하나로 묶는 **중간 계층(Middleware)** 역할을 합니다.\n",
    "\n",
    "| 항목 | LLM API 직접 사용 | LangChain 사용 |\n",
    "|------|------------------|----------------|\n",
    "| 단순 텍스트 생성 | ✅ 가능 | ✅ 가능 |\n",
    "| 외부 데이터 연동 (DB, 파일, API) | ❌ 직접 구현 필요 | ✅ `Retrievers`, `Loaders` 제공 |\n",
    "| 대화 맥락 유지 | ❌ 세션 직접 관리 | ✅ `Memory` 모듈 제공 |\n",
    "| 복합 작업(요약→검색→응답) | ❌ 수작업 연결 | ✅ `Chain`으로 단계적 구성 |\n",
    "| 품질 관리 및 모니터링 | ❌ 불가 | ✅ `LangSmith` 제공 |\n",
    "| 배포 | ❌ 직접 서버 구성 | ✅ `LangServe`로 API 자동화 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 3. 상태와 흐름을 관리할 수 있음\n",
    "\n",
    "LLM은 기본적으로 **Stateless(무상태)** 모델입니다.  \n",
    "LangChain은 여기에 **기억과 흐름**을 추가합니다.\n",
    "\n",
    "### 🔹 주요 기능\n",
    "- **Memory:** 대화 이력, 상태, 사용자 맥락을 자동 저장  \n",
    "- **Chains:** 여러 단계를 순서대로 실행 (예: 문서 요약 → 질의응답 → 보고서 생성)  \n",
    "- **Callbacks:** 실행 과정 모니터링, 로깅, 스트리밍 출력  \n",
    "\n",
    "> 💡 결과적으로, LangChain은 단순한 “텍스트 생성기”가 아닌  \n",
    "> “논리적으로 생각하고 기억하는 AI 시스템”을 만들 수 있게 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 4. 도구(툴) 사용이 가능한 LLM\n",
    "\n",
    "LangChain의 **Agent 시스템**은 LLM이 스스로 적절한 **도구(API, 계산기, 검색기 등)** 를 선택해 사용할 수 있도록 지원합니다.\n",
    "\n",
    "### 예시:\n",
    "> “이번 주 서울 날씨 알려줘.”  \n",
    "> → LangChain Agent가 OpenWeather API를 호출  \n",
    "> → 결과를 LLM에 전달 → 사용자에게 자연어 응답 생성\n",
    "\n",
    "➡️ LLM API 단독으로는 불가능하거나, 직접 복잡하게 구현해야 하는 기능을 프레임워크 차원에서 제공합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 5. 생태계 통합 도구\n",
    "\n",
    "LangChain은 단일 라이브러리가 아니라 **전체 개발·운영 생태계**를 제공합니다.\n",
    "\n",
    "| 구성요소 | 설명 |\n",
    "|-----------|------|\n",
    "| **LangServe** | LangChain 앱을 REST API로 배포 |\n",
    "| **LangSmith** | 응답 품질 테스트 및 디버깅 |\n",
    "| **LangGraph** | 상태 기반(Stateful) LLM 앱을 그래프 형태로 설계 |\n",
    "\n",
    "이를 통해 프로토타입 단계부터 **프로덕션(Production) 수준의 LLM 애플리케이션** 개발이 가능합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 6. LangChain의 한계 (비판적 시각)\n",
    "\n",
    "LangChain이 만능은 아닙니다. 실제 도입 전 다음 사항을 고려해야 합니다.\n",
    "\n",
    "| 항목 | 비판적 시각 |\n",
    "|------|--------------|\n",
    "| **복잡성** | 단순 LLM 호출만 필요한 경우 과도한 구조 |\n",
    "| **성능 이슈** | 체인/에이전트가 많을수록 응답 지연 증가 |\n",
    "| **학습 곡선** | 초보자에겐 구조 이해가 어렵고 진입 장벽 존재 |\n",
    "| **디버깅 난이도** | 자동 연결 구조로 인해 에러 추적이 어려움 |\n",
    "| **LLM 한계 유지** | 환각(Hallucination) 및 비일관성 문제는 여전히 존재 |\n",
    "\n",
    "> 🧩 결론적으로, LangChain은 “LLM의 한계를 보완하지만 완전히 해결하지는 못한다”고 볼 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 7. 정리: 언제 LangChain을 써야 하는가?\n",
    "\n",
    "| 상황 | 적합한 선택 |\n",
    "|------|--------------|\n",
    "| 간단한 챗봇 / 텍스트 생성 | ✅ LLM API 직접 호출 |\n",
    "| 문서 기반 질의응답, 데이터 검색, 다단계 로직 | ✅ LangChain 필요 |\n",
    "| 장기 대화, 상태 유지형 시스템 | ✅ LangChain 필요 |\n",
    "| 품질 모니터링, 배포, 복합 AI 시스템 | ✅ LangChain 필요 |\n",
    "\n",
    "> 📌 **요약:**  \n",
    "> - LLM API → 단일 기능 중심  \n",
    "> - LangChain → **앱 수준의 지능형 시스템 구현**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 결론\n",
    "\n",
    "LangChain은 LLM을 단순한 “언어 모델”이 아니라  \n",
    "**데이터, 논리, 기억, 도구 사용이 가능한 완전한 AI 시스템으로 확장**시켜 줍니다.\n",
    "\n",
    "하지만 모든 프로젝트에 필요한 것은 아니며,  \n",
    "**프로젝트 복잡도와 유지보수 비용을 고려하여 선택적으로 사용하는 것이 합리적입니다.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8088c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n-jison/Desktop/jaeig/kmu-agent-course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd51045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\",\n",
    "                temperature=0.1, \n",
    "                openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e40147bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고래가 바다에 사는 이유는 여러 가지가 있는데, 주로 다음과 같은 이유 때문입니다:\n",
      "\n",
      "1. **적합한 서식 환경**: 고래는 크고 강한 몸집을 가지고 있어 육지보다 넓고 깊은 바다에서 생활하는 것이 더 적합합니다. 바다는 먹이와 서식지가 풍부하며, 고래가 필요로 하는 공간을 제공합니다.\n",
      "\n",
      "2. **먹이 공급**: 고래는 주로 플랑크톤, 작은 물고기, 크릴 등 바다에 서식하는 먹이를 먹습니다. 바다에서 살면 이러한 먹이를 쉽게 찾을 수 있습니다.\n",
      "\n",
      "3. **진화적 역사**: 고래는 육상 포유류에서 진화했지만, 수백만 년 전 바다로 돌아와 적응하며 살아왔습니다. 이 과정에서 바다 환경에 적합한 신체 구조와 생태적 특성을 갖추게 되었습니다.\n",
      "\n",
      "4. **생태적 적응**: 고래는 수영에 적합한 유선형 몸체와 지느러미, 꼬리지느러미 등을 갖추고 있어 바다에서 효율적으로 이동하고 먹이를 잡을 수 있습니다. 육지보다 바다에서 더 잘 적응할 수 있는 생물입니다.\n",
      "\n",
      "요약하자면, 고래는 바다의 환경과 먹이, 그리고 진화적 적응 덕분에 바다에서 살게 된 것입니다.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"고래는 왜 바다에 살까요?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85060c6",
   "metadata": {},
   "source": [
    "### 🧩 기본 LLM 체인의 구성 요소 및 작동 방식\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ 기본 LLM 체인의 구성 요소\n",
    "\n",
    "#### 🔹 프롬프트 (Prompt)\n",
    "- 사용자 또는 시스템에서 제공하는 입력으로, LLM에게 특정 작업을 수행하도록 요청하는 **지시문**입니다.  \n",
    "- 프롬프트는 질문, 명령, 문장 시작 부분 등 다양한 형태를 취할 수 있으며,  \n",
    "  LLM의 응답을 유도하는 데 중요한 역할을 합니다.\n",
    "\n",
    "#### 🔹 LLM (Large Language Model)\n",
    "- **GPT, Gemini** 등 대규모 언어 모델을 의미합니다.  \n",
    "- 대량의 텍스트 데이터를 학습하여 언어를 이해하고 생성할 수 있는 인공지능 시스템입니다.  \n",
    "- LLM은 프롬프트를 바탕으로 적절한 응답을 생성하거나, 주어진 작업을 수행하는 데 사용됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ 일반적인 작동 방식\n",
    "\n",
    "#### 1. 프롬프트 생성\n",
    "- 사용자의 요구 사항이나 특정 작업을 정의하는 프롬프트를 생성합니다.  \n",
    "- 이 프롬프트는 LLM에게 전달되기 전에, 작업의 목적과 맥락을 명확히 전달하기 위해 **최적화될 수 있습니다.**\n",
    "\n",
    "#### 2. LLM 처리\n",
    "- LLM은 제공된 프롬프트를 분석하고, 학습된 지식을 바탕으로 **적절한 응답을 생성**합니다.  \n",
    "- 내부적으로 다양한 언어 패턴과 내외부 지식을 활용하여 요청된 작업을 수행하거나 정보를 제공합니다.\n",
    "\n",
    "#### 3. 응답 반환\n",
    "- LLM에 의해 생성된 응답은 최종 사용자에게 필요한 형태로 변환되어 제공됩니다.  \n",
    "- 이 응답은 **직접적인 답변, 생성된 텍스트, 요약된 정보** 등 다양한 형태를 취할 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "692b5b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"추천메뉴\": \"김치찌개와 밥, 그리고 계란말이 어떠세요? 따뜻하고 맛있는 한 끼가 될 거예요!\"\n",
      "}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "## llm + chattemplate 적용 chain 구성\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "### 지시사항\n",
    "너는 친절한 조수야. 사용자가 무슨 말을 하든 항상 예의 바르게 대답해야 해.\n",
    "### 사용자 입력\n",
    "{user_input}\n",
    "JSON 형태로 반환해줘.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(prompt)\n",
    "chat_prompt\n",
    "\n",
    "chain = chat_prompt | llm\n",
    "response = chain.invoke({\"user_input\": \"오늘 점심 메뉴 아무거나 임의로 추천해줘\"})\n",
    "print(response.content)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c9e2a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'추천메뉴': '김치찌개와 밥, 그리고 계란말이 어떠세요? 따뜻하고 맛있게 드실 수 있을 거예요!'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "## llm + chattemplate + outputparser 적용 chain 구성\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# output_parser = StrOutputParser() ## 구조화된 형태로 반환\n",
    "output_parser = JsonOutputParser() ## JSON 형태로 반환\n",
    "chain = chat_prompt | llm | output_parser ## chain 구성 prompt -> llm -> outputparser\n",
    "response = chain.invoke({\"user_input\": \"오늘 점심 메뉴 아무거나 임의로 추천해줘\"}) ## invoke, batch, stream 존재\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f34babc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 오늘 점심으로는 따뜻한 김치찌개와 고소한 제육볶음을 추천드릴게요. 또는 가볍게 샐러드와 샌드위치도 좋은 선택이 될 것 같아요. 어떤 스타일의 식사를 원하시는지 말씀해주시면 더 맞춤 추천해드릴 수 있어요!"
     ]
    }
   ],
   "source": [
    "stream = chain.stream({\"user_input\": \"오늘 점심 메뉴 아무거나 임의로 추천해줘\"}) ## invoke, batch, stream 존재\n",
    "\n",
    "for i in stream:\n",
    "    print(i, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd821d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Template 예제\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의\n",
    "template_text = \"안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n",
    "\n",
    "filled_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a9f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## chat template 예제\n",
    "# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "\n",
    "messages\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
